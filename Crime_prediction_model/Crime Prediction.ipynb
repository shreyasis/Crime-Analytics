{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import shapefile as shp\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from osgeo import ogr\n",
    "import time\n",
    "import urllib.request\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely.speedups\n",
    "GRID_AREA = 5\n",
    "UNIT = 'miles'   #'km' or 'miles'\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "# settings for number of hotspots need to be predicted\n",
    "HOTSPOT_COUNT = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n"
     ]
    }
   ],
   "source": [
    "def main(outputGridfn):\n",
    "    shapefile = \"./county_boundary/county_boundary.shp\"\n",
    "    driver = ogr.GetDriverByName(\"ESRI Shapefile\")    #eg: GeoJSON, ESRI\n",
    "    dataSource = driver.Open(shapefile, 0)\n",
    "    layer = dataSource.GetLayer()\n",
    "    xmin,xmax,ymin,ymax = layer.GetExtent()\n",
    "    if UNIT == 'km':\n",
    "        gridWidth = gridHeight = math.sqrt(GRID_AREA)*0.009\n",
    "    if UNIT == 'miles':\n",
    "        gridWidth = gridHeight = math.sqrt(GRID_AREA)*0.0145\n",
    "\n",
    "    # get rows\n",
    "    rows = math.ceil((ymax-ymin)/gridHeight)\n",
    "    # get columns\n",
    "    cols = math.ceil((xmax-xmin)/gridWidth)\n",
    "\n",
    "    # start grid cell envelope\n",
    "    ringXleftOrigin = xmin\n",
    "    ringXrightOrigin = xmin + gridWidth\n",
    "    ringYtopOrigin = ymax\n",
    "    ringYbottomOrigin = ymax-gridHeight\n",
    "\n",
    "    # create output file\n",
    "    outDriver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "    if os.path.exists(outputGridfn):\n",
    "        os.remove(outputGridfn)\n",
    "    outDataSource = outDriver.CreateDataSource(outputGridfn)\n",
    "    outLayer = outDataSource.CreateLayer(outputGridfn,geom_type=ogr.wkbMultiPolygon)\n",
    "    featureDefn = outLayer.GetLayerDefn()\n",
    "\n",
    "    # create grid cells\n",
    "    countcols = 0\n",
    "    while countcols < cols:\n",
    "        countcols += 1\n",
    "        print('.........')\n",
    "        # reset envelope for rows\n",
    "        ringYtop = ringYtopOrigin\n",
    "        ringYbottom =ringYbottomOrigin\n",
    "        countrows = 0\n",
    "\n",
    "        while countrows < rows:\n",
    "            countrows += 1\n",
    "            ring = ogr.Geometry(ogr.wkbLinearRing)\n",
    "            ring.AddPoint(ringXleftOrigin, ringYtop)\n",
    "            ring.AddPoint(ringXrightOrigin, ringYtop)\n",
    "            ring.AddPoint(ringXrightOrigin, ringYbottom)\n",
    "            ring.AddPoint(ringXleftOrigin, ringYbottom)\n",
    "#             ring.AddPoint(ringXleftOrigin, ringYtop)\n",
    "            poly = ogr.Geometry(ogr.wkbPolygon)\n",
    "            poly.AddGeometry(ring)\n",
    "\n",
    "            # add new geom to layer\n",
    "            outFeature = ogr.Feature(featureDefn)\n",
    "            outFeature.SetGeometry(poly)\n",
    "            outLayer.CreateFeature(outFeature)\n",
    "            outFeature.Destroy\n",
    "\n",
    "            # new envelope for next poly\n",
    "            ringYtop = ringYtop - gridHeight\n",
    "            ringYbottom = ringYbottom - gridHeight\n",
    "\n",
    "        # new envelope for next poly\n",
    "        ringXleftOrigin = ringXleftOrigin + gridWidth\n",
    "        ringXrightOrigin = ringXrightOrigin + gridWidth\n",
    "\n",
    "    # Close DataSources\n",
    "    outDataSource.Destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #\n",
    "    # example run : $ python grid.py <full-path><output-shapefile-name>.shp xmin xmax ymin ymax gridHeight gridWidth\n",
    "    #\n",
    "\n",
    "    main(\"./Square_grid/new_grid.shp\")\n",
    "    with open(\"./county_boundary/county_boundary.prj\") as f:\n",
    "        with open(\"./Square_grid/new_grid.prj\", \"w\") as f1:\n",
    "            for line in f:\n",
    "                f1.write(line)\n",
    "\n",
    "    f.close()\n",
    "    f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clipping out the required map from Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging started....\n",
      "merging ended....\n"
     ]
    }
   ],
   "source": [
    "shapely.speedups.enable()\n",
    "# File paths\n",
    "border_fp = \"./county_boundary/county_boundary.shp\"\n",
    "grid_fp = \"./Square_Grid/new_grid.shp\"\n",
    "# Read files\n",
    "gridd = gpd.read_file(grid_fp)\n",
    "denv = gpd.read_file(border_fp)\n",
    "print(\"merging started....\")\n",
    "result = gpd.overlay(gridd,denv, how='intersection')\n",
    "print(\"merging ended....\")\n",
    "\n",
    "outfp = \"./clipped_grid/overlay_analysis.shp\"\n",
    "# Use Shapefile driver\n",
    "result.to_file(outfp, driver=\"Shapefile\")\n",
    "\n",
    "with open(\"./county_boundary/county_boundary.prj\") as f:\n",
    "    with open(\"./clipped_grid/overlay_analysis.prj\", \"w\") as f1:\n",
    "        for line in f:\n",
    "            f1.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Output as CSV of Grid Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done area adding\n"
     ]
    }
   ],
   "source": [
    "shapefile = \"./clipped_grid/overlay_analysis.shp\"\n",
    "driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "dataSource = driver.Open(shapefile, 1)\n",
    "layer = dataSource.GetLayer()\n",
    "new_field1 = ogr.FieldDefn(\"AREA\", ogr.OFTReal)\n",
    "new_field2 = ogr.FieldDefn(\"xmin\", ogr.OFTReal)\n",
    "new_field3 = ogr.FieldDefn(\"xmax\", ogr.OFTReal)\n",
    "new_field4 = ogr.FieldDefn(\"ymin\", ogr.OFTReal)\n",
    "new_field5 = ogr.FieldDefn(\"ymax\", ogr.OFTReal)\n",
    "layer.CreateField(new_field1)\n",
    "layer.CreateField(new_field2)\n",
    "layer.CreateField(new_field3)\n",
    "layer.CreateField(new_field4)\n",
    "layer.CreateField(new_field5)\n",
    "\n",
    "for feature in layer:\n",
    "    geom = feature.GetGeometryRef()\n",
    "    # print(geom)\n",
    "    if UNIT == 'km':\n",
    "        area = (geom.GetArea()/(math.pow( (math.sqrt(GRID_AREA)*0.009), 2)))*GRID_AREA # real area adjusted to scale\n",
    "    if UNIT == 'miles':\n",
    "        area = (geom.GetArea()/(math.pow( (math.sqrt(GRID_AREA)*0.0145), 2)))*GRID_AREA # real area adjusted to scale\n",
    "    xmin, xmax, ymin, ymax = geom.GetEnvelope()\n",
    "    feature.SetField(\"AREA\", area)\n",
    "    feature.SetField(\"xmin\", xmin)\n",
    "    feature.SetField(\"xmax\", xmax)\n",
    "    feature.SetField(\"ymin\", ymin)\n",
    "    feature.SetField(\"ymax\", ymax)\n",
    "    layer.SetFeature(feature)\n",
    "# dataSource = None\n",
    "print('done area adding')\n",
    "shpfile=r'./clipped_grid/overlay_analysis.shp' #sys.argv[1]\n",
    "csvfile=r'grid_details.csv' #sys.argv[2]\n",
    "\n",
    "#Open files\n",
    "csvfile=open(csvfile,'wt')\n",
    "ds=ogr.Open(shpfile)\n",
    "lyr=ds.GetLayer()\n",
    "#Get field names\n",
    "dfn=lyr.GetLayerDefn()\n",
    "nfields=dfn.GetFieldCount()\n",
    "fields=[]\n",
    "for i in range(nfields):\n",
    "    fields.append(dfn.GetFieldDefn(i).GetName())\n",
    "csvwriter = csv.DictWriter(csvfile, fields)\n",
    "try:csvwriter.writeheader() #python 2.7+\n",
    "except:csvfile.write(','.join(fields)+'\\n')\n",
    "\n",
    "# Write attributes and kml out to csv\n",
    "for feat in lyr:\n",
    "    attributes=feat.items()\n",
    "    geom=feat.GetGeometryRef()\n",
    "    csvwriter.writerow(attributes)\n",
    "\n",
    "#clean up\n",
    "del csvwriter,lyr,ds\n",
    "csvfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime_CSV to SHP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = './crime_points/crime_points.shp'\n",
    "\n",
    "#Set up blank lists for data\n",
    "c,x,y,=[],[],[]\n",
    "\n",
    "\n",
    "crime_df1 = pd.read_csv(\"./denver_cleaned.csv\")\n",
    "crime_df1['case_id']=crime_df1.index\n",
    "crime_df = crime_df1[['case_id','Longitude','Latitude']]\n",
    "\n",
    "for i, row in crime_df.iterrows():\n",
    "    c.append(row[0])\n",
    "    x.append(float(row[1]))\n",
    "    y.append(float(row[2]))\n",
    "# print(x)\n",
    "\n",
    "#Set up shapefile writer and create empty fields\n",
    "w = shp.Writer('./crime_points/crime_points',shp.POINT,1)\n",
    "# w.point\n",
    "w.autoBalance = 1 #ensures gemoetry and attributes match\n",
    "w.field('CaseID','N')\n",
    "w.field('X','F',11,8)\n",
    "w.field('Y','F',11,8)\n",
    "\n",
    "#loop through the data and write the shapefile\n",
    "for j,k in enumerate(x):\n",
    "    w.point(k,y[j]) #write the geometry\n",
    "    w.record(c[j],k,y[j]) #write the attributes\n",
    "    print(\"...\")\n",
    "\n",
    "with open(\"./county_boundary/county_boundary.prj\") as f:\n",
    "    with open(\"./crime_points/crime_points.prj\", \"w\") as f1:\n",
    "        for line in f:\n",
    "            f1.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Crime points with Clipped Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging started....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ANACONDA\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging ended....\n"
     ]
    }
   ],
   "source": [
    "shapely.speedups.enable()\n",
    "# Read files\n",
    "final_cols=['FID', 'geometry']\n",
    "final = gpd.GeoDataFrame.from_file('./clipped_grid/overlay_analysis.shp')\n",
    "final=final[final_cols]\n",
    "point = gpd.GeoDataFrame.from_file('./crime_points/crime_points.shp')\n",
    "# print(final.head(5))\n",
    "# print(point.head(5))\n",
    "print(\"merging started....\")\n",
    "result = gpd.sjoin(final,point, op='intersects',how='right')\n",
    "print(\"merging ended....\")\n",
    "# print(result.head(5))\n",
    "outfp = \"./points_grid/result_grid.shp\"\n",
    "# # Use Shapefile driver\n",
    "result.to_file(outfp, driver=\"Shapefile\")\n",
    "\n",
    "with open(\"./county_boundary/county_boundary.prj\") as f:\n",
    "    with open(\"./points_grid/result_grid.prj\", \"w\") as f1:\n",
    "        for line in f:\n",
    "            f1.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "shpfile=r'./points_grid/result_grid.shp' #sys.argv[1]\n",
    "csvfile=r'test.csv' #sys.argv[2]\n",
    "\n",
    "\n",
    "#Open files\n",
    "# FID ==> GRID_ID\n",
    "# id ==> CASE_ID\n",
    "csvfile=open(csvfile,'wt')\n",
    "ds = ogr.Open(shpfile)\n",
    "lyr = ds.GetLayer()\n",
    "\n",
    "#Get field names\n",
    "dfn = lyr.GetLayerDefn()\n",
    "nfields = dfn.GetFieldCount()\n",
    "fields=[]\n",
    "for i in range(nfields):\n",
    "    fields.append(dfn.GetFieldDefn(i).GetName())\n",
    "csvwriter = csv.DictWriter(csvfile, fields)\n",
    "try:csvwriter.writeheader()\n",
    "except:csvfile.write(','.join(fields)+'\\n')\n",
    "\n",
    "# Write attributes out to csv\n",
    "for feat in lyr:\n",
    "    attributes=feat.items()\n",
    "    geom=feat.GetGeometryRef()\n",
    "    csvwriter.writerow(attributes)\n",
    "\n",
    "#clean up\n",
    "del csvwriter,lyr,ds\n",
    "csvfile.close()\n",
    "\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "df = df.where((pd.notnull(df)), None)\n",
    "df1=df[['FID','CaseID']]\n",
    "df1.columns=['GridId','CaseId']\n",
    "\n",
    "df1.to_csv(r'./output_final.csv',header=True,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Window Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time: 9.359074354171753\n"
     ]
    }
   ],
   "source": [
    "# time stamping the data with different time window, e.g., weekly, fortnight, monthly, quarterly, half-yearly\n",
    "def calculate_timewindowOverAllYears(day, month, year, min_year, min_month, time_window):\n",
    "    '''\n",
    "    function:\n",
    "    input:\n",
    "    output:\n",
    "    '''\n",
    "    day = int(day)\n",
    "    month = int(month)\n",
    "    year = int(year)\n",
    "    min_year = int(min_year)\n",
    "    min_month = int(min_month)\n",
    "\n",
    "    #time_window: daily\n",
    "    if time_window == 'day':\n",
    "        a = date(int(year),int(month),int(day))\n",
    "        b = date(int(min_year),1,1)\n",
    "        tw = (a-b).days + 1\n",
    "\n",
    "    #time_window: weekly\n",
    "    if time_window == 'week':\n",
    "        if day<=7:\n",
    "            tw = 1 + 4*((month-1) + 12*(year-min_year))\n",
    "        elif day<=14:\n",
    "            tw = 2 + 4*((month-1) + 12*(year-min_year))\n",
    "        elif day<=21:\n",
    "            tw = 3 + 4*((month-1) + 12*(year-min_year))\n",
    "        else:\n",
    "            tw = 4 + 4*((month-1) + 12*(year-min_year))\n",
    "\n",
    "    # time_window: fortnight\n",
    "    if time_window == 'fortnight':\n",
    "        if day<=15:\n",
    "            tw = 1 + 2*((month-1) + 12*(year-min_year))\n",
    "        else:\n",
    "            tw = 2 + 2*((month-1) + 12*(year-min_year))\n",
    "\n",
    "    #time_window: monthly\n",
    "    if time_window == 'month':\n",
    "        tw = (month - min_month) + 12*(year - min_year)\n",
    "\n",
    "    #time_window: quarterly\n",
    "    if time_window == 'quarter':\n",
    "        if month<=3:\n",
    "            tw = 1 + 4*(year - min_year)\n",
    "        elif month<=6:\n",
    "            tw = 2 + 4*(year - min_year)\n",
    "        elif month<=9:\n",
    "            tw = 3 + 4*(year - min_year)\n",
    "        else:\n",
    "            tw = 4 + 4*(year - min_year)\n",
    "\n",
    "    #time_window: half-yearly\n",
    "    if time_window == 'half-year':\n",
    "        if month<=6:\n",
    "            tw = 1 + 2*(year - min_year)\n",
    "        else:\n",
    "            tw = 2 + 2*(year - min_year)\n",
    "\n",
    "    return int(tw)\n",
    "\n",
    "\n",
    "s_t=time.time()\n",
    "cases = pd.read_csv('./denver_cleaned.csv')\n",
    "cases['CaseId'] = cases.index\n",
    "\n",
    "time_window = 'month'\n",
    "min_year = int(min(cases['Year']))\n",
    "min_month = int(min(cases[cases.Year == min_year].Month))\n",
    "\n",
    "cases['MonthId'] = cases.apply(lambda row : calculate_timewindowOverAllYears(row['Day'], row['Month'], row['Year'], min_year, min_month, time_window),axis=1)\n",
    "\n",
    "# reads the gridid mapped cases in case_grid_df dataframe\n",
    "# conn = pyodbc.connect('DRIVER={' + DB_DRIVER + '};'\n",
    "#                       'SERVER=' + LOCAL_SERVER + ';'\n",
    "#                       'DATABASE=' + FEATURE_PREPROCESSING_DB + ';'\n",
    "#                       'UID=' + LOCAL_SERVER_UID + ';'\n",
    "#                       'PWD=' + LOCAL_SERVER_PWD + ';'\n",
    "#                       )\n",
    "# case_grid_df = pd.read_sql('SELECT * FROM CasesGridTwMapped', con = conn)\n",
    "case_grid_df=pd.read_csv('./output_final.csv')\n",
    "caseid_gridid_mapping = case_grid_df[['CaseId', 'GridId']]\n",
    "casees_complete =pd.merge(cases, caseid_gridid_mapping, on='CaseId')\n",
    "\n",
    "caseid_monthid_gridid = casees_complete[['CaseId', 'MonthId', 'GridId']]\n",
    "caseid_monthid_gridid = caseid_monthid_gridid.where((pd.notnull(caseid_monthid_gridid)), None)\n",
    "\n",
    "caseid_monthid_gridid.to_csv(r'./grid_tw_mapped.csv',header=True,index=None)\n",
    "e_t = time.time()\n",
    "exe_time = e_t - s_t\n",
    "print('execution time: '+str(exe_time))\n",
    "\n",
    "# create the tw_mapping frame and save that TwMapping table in front_db\n",
    "tw_mapping = casees_complete[['Year', 'Month', 'MonthId']].drop_duplicates()\n",
    "# add an extra month for the (max_month+1) used by prediction module\n",
    "row_monthid_max = tw_mapping.loc[tw_mapping['MonthId'].idxmax()]\n",
    "max_monthid = int(row_monthid_max[2])\n",
    "max_month = int(row_monthid_max[1])\n",
    "max_year = int(row_monthid_max[0])\n",
    "if max_month == 12:\n",
    "    future_month = 1\n",
    "    future_year = max_year + 1\n",
    "else:\n",
    "    future_month = max_month + 1\n",
    "    future_year = max_year\n",
    "future_monthid = max_monthid + 1\n",
    "\n",
    "tw_mapping = tw_mapping.append({'Year':future_year, 'Month':future_month, 'MonthId':future_monthid}, ignore_index=True)\n",
    "\n",
    "tw_mapping.to_csv(r'./front_db_tw.csv',index=None,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating GridId CaseId and MonthId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cases space-time aggregation done and stored in feature_preprocessing_db GridTwCrimeAggregated csv\n",
      "execution time: 0.03390836715698242\n"
     ]
    }
   ],
   "source": [
    "# create crime dataframe aggregated over specified time_window, e.g., weekly, fortnight, monthly, quarterly etc.\n",
    "def aggregate_crime_data_grid_timewindow(crime_points, grids):\n",
    "    '''\n",
    "    function:\n",
    "    input:\n",
    "    output:\n",
    "    '''\n",
    "    crime_points_trimmed = crime_points[['GridId', 'MonthId']]\n",
    "    X =  crime_points_trimmed.groupby(['GridId','MonthId'])['GridId'].count().to_frame()\n",
    "\n",
    "    X.columns = ['CrimeCount']\n",
    "    X = X.reset_index()\n",
    "\n",
    "    # preparing the time_window aggregated crime data and grid data for joining\n",
    "    tws = pd.DataFrame(data = X['MonthId'].unique(),columns=['MonthId'])\n",
    "\n",
    "    grids['key'] = 0\n",
    "    tws['key'] = 0\n",
    "    grids = grids.merge(tws, how='outer')\n",
    "\n",
    "    data = X.set_index(['GridId','MonthId'])\n",
    "    grids_temp = grids.set_index(['GridId','MonthId'])\n",
    "\n",
    "    # joining the crime data and grid data to get the grid area in crime data. this area is used for calculating the normalized crime\n",
    "    EntireFrame = data.join(grids_temp,how='outer')\n",
    "    EntireFrame = EntireFrame.fillna(0)\n",
    "    EntireFrame = EntireFrame[['CrimeCount','Area']]\n",
    "    EntireFrame.reset_index(inplace=True)\n",
    "    EntireFrame = EntireFrame[EntireFrame['Area']>0]\n",
    "    EntireFrame['NormCrimeCount'] = EntireFrame['CrimeCount'] * (GRID_AREA /EntireFrame['Area'])\n",
    "    return EntireFrame\n",
    "\n",
    "\n",
    "s_t = time.time()\n",
    "grid_info = pd.read_csv('./grid_details.csv')\n",
    "grid_info = grid_info.rename(columns={'FID' : 'GridId', 'AREA' : 'Area'})\n",
    "cases_grid_tw_mapped = pd.read_csv('./grid_tw_mapped.csv')\n",
    "cases_grid_tw_mapped.replace(to_replace='', value=np.nan)\n",
    "cases_grid_tw_mapped= cases_grid_tw_mapped[np.isfinite(cases_grid_tw_mapped['GridId'])]\n",
    "\n",
    "\n",
    "# aggregates the cases based on spece and time\n",
    "entire_frame = aggregate_crime_data_grid_timewindow(cases_grid_tw_mapped, grid_info)\n",
    "\n",
    "s_t=time.time()\n",
    "\n",
    "entire_frame.to_csv(r'./grid_tw_crime_aggregated.csv',index=None,header=True)\n",
    "print('cases space-time aggregation done and stored in feature_preprocessing_db GridTwCrimeAggregated csv')\n",
    "e_t = time.time()\n",
    "exe_time = e_t - s_t\n",
    "print('execution time: '+str(exe_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly mean number of crimes\n",
    "def MeanNumberOfCrimes(frame):\n",
    "    \"\"\"\n",
    "    function:\n",
    "    input:\n",
    "    output:\n",
    "    \"\"\"\n",
    "    return frame[['id', 'Count']].groupby(['id']).mean()\n",
    "\n",
    "\n",
    "# monthly normalized number of crimes\n",
    "def NormalizedNumberOfCrimes(frame):\n",
    "    \"\"\"\n",
    "    function:\n",
    "    input:\n",
    "    output:\n",
    "    \"\"\"\n",
    "    df = frame[['id', 'Count']].groupby(['id']).sum()\n",
    "    return df / df['Count'].max()\n",
    "\n",
    "\n",
    "# rank based on monthly normalized number of crimes\n",
    "def RankGrids(frame):\n",
    "    \"\"\"\n",
    "    function:\n",
    "    input:\n",
    "    output:\n",
    "    \"\"\"\n",
    "    df = NormalizedNumberOfCrimes(frame)\n",
    "    df['rank'] = df.rank(ascending=False)\n",
    "    return df['rank']\n",
    "\n",
    "\n",
    "# hotspot frequency of the grid\n",
    "def HotspotFrequency(frame, numberOfHotspots):\n",
    "    \"\"\"\n",
    "    function:\n",
    "    input:\n",
    "    output:\n",
    "    \"\"\"\n",
    "    df = frame.groupby('id')['HotSpot'].sum().to_frame()\n",
    "    return df\n",
    "\n",
    "\n",
    "# hotspot history\n",
    "def HotspotHistory(frame, numberOfHotspots):\n",
    "    \"\"\"\n",
    "    function:\n",
    "    input:\n",
    "    output:\n",
    "    \"\"\"\n",
    "    df = HotspotFrequency(frame, numberOfHotspots)\n",
    "    df['HotSpot'] = np.where(df['HotSpot'] > 0, 1, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "# total crimes in the neighbors in last 1 month\n",
    "def CrimesNeighborSum_1(row, frame):\n",
    "    \"\"\"\n",
    "    function:\n",
    "    input:\n",
    "    output:\n",
    "    \"\"\"\n",
    "    # getting the neighbor ids for a grid\n",
    "    neighboringIds = row['NEIGHBORS'].split(\",\")\n",
    "    # getting the rows corresponding to the neighbors in 1 month prior\n",
    "    neighborRows = frame[\n",
    "        (frame['twOverAllYears'] == row['twOverAllYears'] - 1) & (frame['id'].isin(neighboringIds))]\n",
    "    return sum(neighborRows['Count'])\n",
    "\n",
    "\n",
    "# total crimes in the neighbors in last 2 month\n",
    "def CrimesNeighborSum_2(row, frame):\n",
    "    \"\"\"\n",
    "    function:\n",
    "    input:\n",
    "    output:\n",
    "    \"\"\"\n",
    "    # getting the neighbor ids for a grid\n",
    "    neighboringIds = row['NEIGHBORS'].split(\",\")\n",
    "    # getting the rows corresponding to the neighbors in 1 month prior\n",
    "    neighborRows1 = frame[\n",
    "        (frame['twOverAllYears'] == row['twOverAllYears'] - 1) & (frame['id'].isin(neighboringIds))]\n",
    "    # getting the rows corresponding to the neighbors in 2 month prior\n",
    "    neighborRows2 = frame[\n",
    "        (frame['twOverAllYears'] == row['twOverAllYears'] - 2) & (frame['id'].isin(neighboringIds))]\n",
    "    return sum(neighborRows1['Count']) + sum(neighborRows2['Count'])\n",
    "\n",
    "\n",
    "# total crimes in the neighbors in last 3 month\n",
    "def CrimesNeighborSum_3(row, frame):\n",
    "    \"\"\"\n",
    "    function:\n",
    "    input:\n",
    "    output:\n",
    "    \"\"\"\n",
    "    # getting the neighbor ids for a grid\n",
    "    neighboringIds = row['NEIGHBORS'].split(\",\")\n",
    "    # getting the rows corresponding to the neighbors in 1 month prior\n",
    "    neighborRows1 = frame[\n",
    "        (frame['twOverAllYears'] == row['twOverAllYears'] - 1) & (frame['id'].isin(neighboringIds))]\n",
    "    # getting the rows corresponding to the neighbors in 2 month prior\n",
    "    neighborRows2 = frame[\n",
    "        (frame['twOverAllYears'] == row['twOverAllYears'] - 2) & (frame['id'].isin(neighboringIds))]\n",
    "    # getting the rows corresponding to the neighbors in 3 month prior\n",
    "    neighborRows3 = frame[\n",
    "        (frame['twOverAllYears'] == row['twOverAllYears'] - 3) & (frame['id'].isin(neighboringIds))]\n",
    "    return sum(neighborRows1['Count']) + sum(neighborRows2['Count']) + sum(neighborRows3['Count'])\n",
    "\n",
    "\n",
    "# feature engineering for the crime dataset\n",
    "def feature_engineering(EntireFrame, number_of_hotspots):\n",
    "    \"\"\"\n",
    "    function:\n",
    "    input:\n",
    "    output:\n",
    "    \"\"\"\n",
    "    # getting the hotspot crime dataframe\n",
    "    HotSpotFrame = AddHotSpotColumnToData(EntireFrame, number_of_hotspots)\n",
    "\n",
    "    # getting the max and min time_window in the dataset\n",
    "    twMax = int(EntireFrame['twOverAllYears'].max())\n",
    "    twMin = int(EntireFrame['twOverAllYears'].min()) + 3\n",
    "\n",
    "    # seting the month window for calculating the statistical features for the crime data points\n",
    "    windows = [1, 2, 3]\n",
    "    EntireFeatureFrame = pd.DataFrame()\n",
    "    for tw in range(twMin, twMax + 1):\n",
    "        features = pd.DataFrame(data=EntireFrame['id'].unique(), columns=['id'])\n",
    "        for i in windows:\n",
    "            frame = EntireFrame[\n",
    "                (EntireFrame['twOverAllYears'] >= tw - i) & (EntireFrame['twOverAllYears'] < tw)]\n",
    "            hsframe = HotSpotFrame[\n",
    "                (HotSpotFrame['twOverAllYears'] >= tw - i) & (HotSpotFrame['twOverAllYears'] < tw)]\n",
    "\n",
    "            features = features.merge(MeanNumberOfCrimes(frame).reset_index(), left_on='id', right_on='id')\n",
    "            features = features.rename(columns={'Count': 'mean' + str(i)})\n",
    "            features = features.merge(NormalizedNumberOfCrimes(frame).reset_index(), left_on='id', right_on='id')\n",
    "            features = features.rename(columns={'Count': 'norm' + str(i)})\n",
    "            features = features.merge(RankGrids(frame).reset_index(), left_on='id', right_on='id')\n",
    "            features = features.rename(columns={'rank': 'rank' + str(i)})\n",
    "            features = features.merge(HotspotFrequency(hsframe, number_of_hotspots).reset_index(), left_on='id',\n",
    "                                      right_on='id')\n",
    "            features = features.rename(columns={'HotSpot': 'HotSpot' + str(i)})\n",
    "            features = features.merge(HotspotHistory(hsframe, number_of_hotspots).reset_index(), left_on='id',\n",
    "                                      right_on='id')\n",
    "            features = features.rename(columns={'HotSpot': 'HotSpotFreq' + str(i)})\n",
    "\n",
    "        features['twOverAllYears'] = tw\n",
    "        EntireFeatureFrame = EntireFeatureFrame.append(features)\n",
    "\n",
    "    return EntireFeatureFrame\n",
    "\n",
    "\n",
    "# add hotspot column to dataframe\n",
    "def AddHotSpotColumnToData(frame, numberOfHotspots):\n",
    "    \"\"\"\n",
    "    function:\n",
    "    input:\n",
    "    output:\n",
    "    \"\"\"\n",
    "    frame['HotSpot'] = 0\n",
    "    # calculating the hotspot threshold value for specific month\n",
    "    thresholdValues = frame.groupby('twOverAllYears').apply(\n",
    "        lambda grp: grp.sort_values(by='Normalized Crimes', ascending=False).iloc[numberOfHotspots - 1][\n",
    "            'Normalized Crimes'])\n",
    "    frame['thresh'] = thresholdValues[frame['twOverAllYears']].values\n",
    "    # determing whether a grid is hotspot or not based on the monthly threshold value\n",
    "    frame['HotSpot'] = np.where(frame['Normalized Crimes'] >= frame['thresh'], 1, 0)\n",
    "    return frame[['id', 'twOverAllYears', 'HotSpot']]\n",
    "\n",
    "\n",
    "# bucketing the crime data based on crime count\n",
    "def AddBucketColumnToData(frame, time_window):\n",
    "    \"\"\"\n",
    "    function:\n",
    "    input:\n",
    "    output:\n",
    "    \"\"\"\n",
    "    # time_window: daily\n",
    "    if time_window == 'day':\n",
    "        # buckets for 2 level classification for daily\n",
    "        frame['CrimeBucket_1'] = pd.cut(frame['Count'], [-1, 0, 10000], labels=['zero', 'non-zero'])\n",
    "        frame['CrimeBucket_2'] = pd.cut(frame['Count'], [0, 1, 2, 3, 4, 7, 10000], labels=['1', '2', '3', '4', '5', '6'])\n",
    "\n",
    "    # time_window: weekly\n",
    "    if time_window == 'week':\n",
    "        # buckets for 2 level classification for weekly\n",
    "        frame['CrimeBucket_1'] = pd.cut(frame['Count'], [-1, 0, 10000], labels=['zero', 'non-zero'])\n",
    "        frame['CrimeBucket_2'] = pd.cut(frame['Count'], [0, 3, 5, 9, 13, 33, 10000], labels=['1', '2', '3', '4', '5', '6'])\n",
    "\n",
    "    # time_window: monthlyly\n",
    "    if time_window == 'month':\n",
    "        # buckets for 2 level classification for monthly\n",
    "        frame['CrimeBucket_1'] = pd.cut(frame['Count'], [-1, 0, 10000], labels=['zero', 'non-zero'])\n",
    "        # frame['CrimeBucket_2'] = pd.cut(frame['Count'], [0, 7, 15, 28, 41, 103, 10000], labels=['1', '2', '3', '4', '5', '6'])\n",
    "        frame['CrimeBucket_2'] = pd.cut(frame['Count'], [0, 3, 11, 24, 54, 141, 10000], labels=['1', '2', '3', '4', '5', '6'])\n",
    "\n",
    "    return frame[['id', 'twOverAllYears', 'CrimeBucket_1', 'CrimeBucket_2']]\n",
    "\n",
    "\n",
    "def CrimeInfoPreprocessing(grid_month_crime_info):\n",
    "    grid_month_crime_info.columns = ['id', 'twOverAllYears', 'Count', 'area', 'Normalized Crimes']\n",
    "    grid_month_crime_info['id'] = pd.to_numeric(grid_month_crime_info['id'])\n",
    "    grid_month_crime_info['twOverAllYears'] = pd.to_numeric(grid_month_crime_info['twOverAllYears'])\n",
    "    grid_month_crime_info['Count'] = pd.to_numeric(grid_month_crime_info['Count'])\n",
    "    grid_month_crime_info['area'] = pd.to_numeric(grid_month_crime_info['area'])\n",
    "    grid_month_crime_info['Normalized Crimes'] = pd.to_numeric(grid_month_crime_info['Normalized Crimes'])\n",
    "\n",
    "    return grid_month_crime_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################################################\n",
    "# First level (zero, non-zero) crime volume bucket prediction model building\n",
    "#############################################################################\n",
    "\n",
    "def TrainFirstLevelModel(feature_frame):\n",
    "    feature_frame['Volume_Label'] = feature_frame['CrimeBucket_1']\n",
    "    feature_frame = feature_frame.drop(['CrimeBucket_1', 'CrimeBucket_2'], axis=1)\n",
    "\n",
    "    # put a split timewindow in the total dataset for future train/test split\n",
    "    total_tw = feature_frame.twOverAllYears.unique()\n",
    "    split_tw = math.floor(len(total_tw) * 0.8)\n",
    "\n",
    "    train = feature_frame.loc[feature_frame['twOverAllYears'] <= split_tw]\n",
    "    test = feature_frame.loc[feature_frame['twOverAllYears'] > split_tw]\n",
    "\n",
    "    X_train = train.loc[:, train.columns != 'Volume_Label']\n",
    "    Y_train = train['Volume_Label']\n",
    "    X_test = test.loc[:, test.columns != 'Volume_Label']\n",
    "    Y_test = test['Volume_Label']\n",
    "\n",
    "    # linearizing the data\n",
    "    Y_train = Y_train.ravel()\n",
    "\n",
    "    # fit a XGBoost model to the data\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "    # print(model)\n",
    "\n",
    "    # Test the model on test split\n",
    "    expected = Y_test\n",
    "    predicted = model.predict(X_test)\n",
    "\n",
    "    confusion_matrix = metrics.confusion_matrix(expected, predicted)\n",
    "    classification_report = metrics.classification_report(expected, predicted)\n",
    "    accuracy = metrics.accuracy_score(expected, predicted)\n",
    "    return {\"model\": model,\n",
    "            \"confusion_matrix\": confusion_matrix,\n",
    "            \"classification_report\": classification_report,\n",
    "            \"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# Second level crime volume bucket prediction model building\n",
    "#############################################################\n",
    "\n",
    "\n",
    "def TrainSecondLevelModel(feature_frame):\n",
    "    feature_frame['Volume_Label'] = feature_frame['CrimeBucket_2']\n",
    "    feature_frame = feature_frame.drop(['CrimeBucket_1', 'CrimeBucket_2'], axis=1)\n",
    "    feature_frame = feature_frame[feature_frame['Volume_Label'].notnull()]\n",
    "\n",
    "    # Splitting the featurised crime data into training and test\n",
    "    # put a split timewindow in the total dataset for future train/test split\n",
    "    total_tw = feature_frame.twOverAllYears.unique()\n",
    "    split_tw = math.floor(len(total_tw) * 0.8)\n",
    "\n",
    "    train = feature_frame.loc[feature_frame['twOverAllYears'] <= split_tw]\n",
    "    test = feature_frame.loc[feature_frame['twOverAllYears'] > split_tw]\n",
    "\n",
    "    X_train = train.loc[:, train.columns != 'Volume_Label']\n",
    "    Y_train = train['Volume_Label']\n",
    "    X_test = test.loc[:, test.columns != 'Volume_Label']\n",
    "    Y_test = test['Volume_Label']\n",
    "\n",
    "    # linearizing the data\n",
    "    Y_train = Y_train.ravel()\n",
    "\n",
    "    # fit a XGBoost model to the data\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Test the model on test split\n",
    "    expected = Y_test\n",
    "    predicted = model.predict(X_test)\n",
    "\n",
    "    confusion_matrix = metrics.confusion_matrix(expected, predicted)\n",
    "    classification_report = metrics.classification_report(expected, predicted)\n",
    "    accuracy = metrics.accuracy_score(expected, predicted)\n",
    "    return {\"model\": model,\n",
    "            \"confusion_matrix\": confusion_matrix,\n",
    "            \"classification_report\": classification_report,\n",
    "            \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running crime volume prediction model building service ...\n",
      "     id  mean1     norm1  rank1  HotSpot1  HotSpotFreq1  mean2     norm2  \\\n",
      "0   7.0    2.0  0.005495   52.5         0             0    3.5  0.009915   \n",
      "1   8.0    5.0  0.027473   40.5         1             1   12.0  0.067989   \n",
      "2   9.0    3.0  0.008242   50.5         0             0    4.0  0.011331   \n",
      "3  13.0    4.0  0.010989   48.0         0             0    2.0  0.005666   \n",
      "4  14.0   18.0  0.049451   32.5         0             0   18.5  0.052408   \n",
      "\n",
      "   rank2  HotSpot2  HotSpotFreq2      mean3     norm3  rank3  HotSpot3  \\\n",
      "0   50.5         1             1   3.333333  0.009174   49.0         1   \n",
      "1   29.0         2             1  14.333333  0.078899   28.0         3   \n",
      "2   47.5         0             0   4.666667  0.012844   47.0         0   \n",
      "3   52.5         0             0   2.666667  0.007339   52.0         0   \n",
      "4   34.0         0             0  16.666667  0.045872   36.0         0   \n",
      "\n",
      "   HotSpotFreq3  twOverAllYears  \n",
      "0             1               3  \n",
      "1             1               3  \n",
      "2             0               3  \n",
      "3             0               3  \n",
      "4             0               3  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ANACONDA\\lib\\site-packages\\pandas\\core\\series.py:628: FutureWarning: Categorical.ravel will return a Categorical object instead of an ndarray in a future version.\n",
      "  return self._values.ravel(order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first level model saved to: ./model/volume_prediction_model_first_level.p\n",
      "confusion matrix\n",
      "[[1015    6]\n",
      " [   5  234]]\n",
      "\n",
      "\n",
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    non-zero       1.00      0.99      0.99      1021\n",
      "        zero       0.97      0.98      0.98       239\n",
      "\n",
      "    accuracy                           0.99      1260\n",
      "   macro avg       0.99      0.99      0.99      1260\n",
      "weighted avg       0.99      0.99      0.99      1260\n",
      "\n",
      "accuracy: 0.9912698412698413\n",
      "second level model saved to: ./model/volume_prediction_model_second_level.p\n",
      "confusion matrix\n",
      "[[ 23  20   2   1   0   0]\n",
      " [ 16  58  24   2   1   0]\n",
      " [  0  15 105  29   3   0]\n",
      " [  0   1  53 142  47   1]\n",
      " [  0   0   0  14 292   9]\n",
      " [  0   0   0   0  11 152]]\n",
      "\n",
      "\n",
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.50      0.54        46\n",
      "           2       0.62      0.57      0.59       101\n",
      "           3       0.57      0.69      0.62       152\n",
      "           4       0.76      0.58      0.66       244\n",
      "           5       0.82      0.93      0.87       315\n",
      "           6       0.94      0.93      0.94       163\n",
      "\n",
      "    accuracy                           0.76      1021\n",
      "   macro avg       0.72      0.70      0.70      1021\n",
      "weighted avg       0.76      0.76      0.75      1021\n",
      "\n",
      "accuracy: 0.7561214495592556\n",
      "model building done\n",
      "execution time: 8.23484206199646\n"
     ]
    }
   ],
   "source": [
    "print(\"running crime volume prediction model building service ...\")\n",
    "\n",
    "s_t = time.time()\n",
    "\n",
    "grid_month_crime_info = pd.read_csv('./grid_tw_crime_aggregated.csv')\n",
    "# conn.close()\n",
    "\n",
    "grid_month_crime_info = CrimeInfoPreprocessing(grid_month_crime_info)\n",
    "\n",
    "time_window = 'month'\n",
    "\n",
    "# doing the feature engineering and getting the fully featured crime data\n",
    "EntireFeatureFrame = feature_engineering(grid_month_crime_info, HOTSPOT_COUNT)\n",
    "print(EntireFeatureFrame.head(5))\n",
    "# add bucket label to the crime data\n",
    "crime_bucket_frame = AddBucketColumnToData(grid_month_crime_info, time_window)\n",
    "\n",
    "# getting the hotspot crime dataframe\n",
    "hotspot_frame = AddHotSpotColumnToData(grid_month_crime_info, HOTSPOT_COUNT)\n",
    "\n",
    "# joining the feature_frame with crime_bucket_frame\n",
    "feature_frame = EntireFeatureFrame.copy()\n",
    "feature_frame = feature_frame.merge(crime_bucket_frame, left_on=['id', 'twOverAllYears'], right_on=['id', 'twOverAllYears'])\n",
    "\n",
    "# train first level model\n",
    "first_level_model = TrainFirstLevelModel(feature_frame)\n",
    "\n",
    "# save the model to disk\n",
    "first_level_model_save_file = './model/volume_prediction_model_first_level.p'\n",
    "pickle.dump(first_level_model[\"model\"], open(first_level_model_save_file, 'wb'))\n",
    "\n",
    "print('first level model saved to:', first_level_model_save_file)\n",
    "print('confusion matrix')\n",
    "print(first_level_model[\"confusion_matrix\"])\n",
    "print('\\n')\n",
    "print('classification report')\n",
    "print(first_level_model[\"classification_report\"])\n",
    "print('accuracy: ' + str(first_level_model[\"accuracy\"]))\n",
    "\n",
    "# train first level model\n",
    "second_level_model = TrainSecondLevelModel(feature_frame)\n",
    "\n",
    "# save the model to disk\n",
    "second_level_model_save_file = './model/volume_prediction_model_second_level.p'\n",
    "pickle.dump(second_level_model[\"model\"], open(second_level_model_save_file, 'wb'))\n",
    "\n",
    "print('second level model saved to:', second_level_model_save_file)\n",
    "print('confusion matrix')\n",
    "print(second_level_model[\"confusion_matrix\"])\n",
    "print('\\n')\n",
    "print('classification report')\n",
    "print(second_level_model[\"classification_report\"])\n",
    "print('accuracy: ' + str(second_level_model[\"accuracy\"]))\n",
    "\n",
    "print('model building done')\n",
    "e_t = time.time()\n",
    "exe_time = e_t - s_t\n",
    "print('execution time: '+str(exe_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running crime volume prediction service ...\n",
      "2nd level predictions\n",
      "['2' '3' '3' '2' '2' '3' '5' '5' '5' '3' '3' '3' '1' '3' '6' '6' '6' '5'\n",
      " '3' '3' '6' '6' '6' '5' '3' '5' '6' '5' '5' '1' '2' '5' '5' '5' '5' '5'\n",
      " '5' '5' '5' '5' '2' '3' '5' '6' '5' '5' '5' '5' '5' '5' '3' '4' '2' '3'\n",
      " '5' '3' '4' '5' '1' '1' '4' '4' '2' '4' '4' '1' '1' '1' '2' '3' '1' '1'\n",
      " '3' '1' '3' '3' '1' '1' '1' '1' '1' '1' '1' '1']\n",
      "predictions generated for month 7 2018 ...\n",
      "prediction generation done\n",
      "execution time: 5.2932984828948975\n"
     ]
    }
   ],
   "source": [
    "print(\"running crime volume prediction service ...\")\n",
    "\n",
    "s_t = time.time()\n",
    "\n",
    "grid_month_crime_info = pd.read_csv('./grid_tw_crime_aggregated.csv')\n",
    "\n",
    "twmapping_info = pd.read_csv('./front_db_tw.csv')\n",
    "\n",
    "grid_month_crime_info = CrimeInfoPreprocessing(grid_month_crime_info)\n",
    "max_month = grid_month_crime_info['twOverAllYears'].max()\n",
    "pred_month = max_month + 1\n",
    "\n",
    "# Create rows for next month 'id' and 'area' columns remain same\n",
    "append_rows = grid_month_crime_info[grid_month_crime_info['twOverAllYears'] == max_month]\n",
    "append_rows['twOverAllYears'].values[:] = pred_month\n",
    "append_rows['Count'].values[:] = 0\n",
    "append_rows['Normalized Crimes'].values[:] = 0\n",
    "\n",
    "grid_month_crime_info = grid_month_crime_info.append(append_rows)\n",
    "\n",
    "time_window = 'month'\n",
    "\n",
    "# doing the feature engineering and getting the fully featured crime data\n",
    "EntireFeatureFrame = feature_engineering(grid_month_crime_info, HOTSPOT_COUNT+1)\n",
    "\n",
    "# add bucket label to the crime data\n",
    "crime_bucket_frame = AddBucketColumnToData(grid_month_crime_info, time_window)\n",
    "\n",
    "# getting the hotspot crime dataframe\n",
    "hotspot_frame = AddHotSpotColumnToData(grid_month_crime_info, HOTSPOT_COUNT)\n",
    "\n",
    "# joining the feature_frame with crime_bucket_frame\n",
    "feature_frame = EntireFeatureFrame.copy()\n",
    "feature_frame = feature_frame.merge(crime_bucket_frame, left_on=['id', 'twOverAllYears'], right_on=['id', 'twOverAllYears'])\n",
    "\n",
    "# print(feature_frame.describe())\n",
    "future_feature_frame = feature_frame[feature_frame['twOverAllYears'] == pred_month]\n",
    "future_feature_frame = future_feature_frame.drop(['CrimeBucket_1', 'CrimeBucket_2'], axis=1)\n",
    "\n",
    "# load the first level model from disk\n",
    "first_level_model_save_file = \"./model/volume_prediction_model_first_level.p\"\n",
    "first_level_model = pickle.load(open(first_level_model_save_file, 'rb'))\n",
    "\n",
    "# generate first level predictions\n",
    "first_level_predictions = first_level_model.predict(future_feature_frame)\n",
    "first_level_predictions_proba = first_level_model.predict_proba(future_feature_frame)\n",
    "\n",
    "# load the second level model from disk\n",
    "second_level_model_save_file = \"./model/volume_prediction_model_second_level.p\"\n",
    "second_level_model = pickle.load(open(second_level_model_save_file, 'rb'))\n",
    "\n",
    "# generate second level predictions\n",
    "second_level_predictions = second_level_model.predict(future_feature_frame)\n",
    "second_level_predictions_proba = second_level_model.predict_proba(future_feature_frame)\n",
    "print(\"2nd level predictions\")\n",
    "print(second_level_predictions)\n",
    "# print(second_level_predictions_proba)\n",
    "# merge predictions\n",
    "first_level_predictions = [0 if x == 'zero' else 1 for x in first_level_predictions]\n",
    "second_level_predictions = [int(x) for x in second_level_predictions]\n",
    "preds = [first_level_predictions, second_level_predictions]\n",
    "predictions = [x * y for x, y in zip(*preds)]\n",
    "flipped_first_level_predictions = [(x - 1) * -1 for x in first_level_predictions]\n",
    "future_feature_frame['VolumeBucketPrediction'] = predictions\n",
    "future_feature_frame['VolumeBucketPredictionProba'] = np.amax(first_level_predictions_proba,axis=-1) * flipped_first_level_predictions + np.amax(second_level_predictions_proba, axis=-1) * first_level_predictions\n",
    "\n",
    "mapping_year, mapping_month, _ = tuple(list(twmapping_info[twmapping_info['MonthId'] == pred_month].values[0]))\n",
    "\n",
    "future_feature_frame['Month'] = mapping_month\n",
    "future_feature_frame['Year'] = mapping_year\n",
    "\n",
    "\n",
    "# generate hotspot labels\n",
    "hotspot_thresh = np.sort(np.array(list(future_feature_frame['VolumeBucketPrediction'] + future_feature_frame['VolumeBucketPredictionProba'])))[-HOTSPOT_COUNT]\n",
    "future_feature_frame['HotspotPrediction'] = ((future_feature_frame['VolumeBucketPrediction'] + future_feature_frame['VolumeBucketPredictionProba']) > hotspot_thresh)\n",
    "future_feature_frame['HotspotPrediction'] = future_feature_frame['HotspotPrediction'].astype(int)\n",
    "\n",
    "print(\"predictions generated for month\", mapping_month, mapping_year, \"...\")\n",
    "\n",
    "# drop useless colums\n",
    "future_feature_frame = future_feature_frame[['id', 'Month', 'Year', 'VolumeBucketPrediction', 'VolumeBucketPredictionProba', 'HotspotPrediction']]\n",
    "future_feature_frame.columns = ['GridId', 'Month', 'Year', 'VolumeBucketPrediction', 'VolumeBucketPredictionProba', 'HotspotPrediction']\n",
    "\n",
    "future_feature_frame = future_feature_frame[future_feature_frame.HotspotPrediction == 1]\n",
    "future_feature_frame.to_csv(r'./predictions_final.csv',index=None,header=True)\n",
    "print('prediction generation done')\n",
    "e_t = time.time()\n",
    "exe_time = e_t - s_t\n",
    "print('execution time: '+str(exe_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google TTS Alert system prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required module for text\n",
    "# to speech conversion\n",
    "from gtts import gTTS\n",
    "import playsound\n",
    "\n",
    "# This module is imported so that we can\n",
    "# play the converted audio\n",
    "import os\n",
    "\n",
    "lat = '123'\n",
    "lon = '342'\n",
    "# The text that you want to convert to audio\n",
    "mytext = 'Attention All units tighten security at latitude'+lat+'and longitude'+lon\n",
    "\n",
    "# Language in which you want to convert\n",
    "language = 'en'\n",
    "\n",
    "# Passing the text and language to the engine,\n",
    "# here we have marked slow=False. Which tells\n",
    "# the module that the converted audio should\n",
    "# have a high speed\n",
    "myobj = gTTS(text=mytext, lang=language, slow=False)\n",
    "\n",
    "# Saving the converted audio in a mp3 file named\n",
    "# welcome\n",
    "myobj.save(\"warning.mp3\")\n",
    "playsound.playsound('./warning.mp3', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
